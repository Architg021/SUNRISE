{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjKG7vvkiEo+YNFfmEEES2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Architg021/SUNRISE/blob/main/Sentinment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lXO9TmgB_gum"
      },
      "outputs": [],
      "source": [
        "#!pip install numpy==1.25.2 --force-reinstall\n",
        "#!pip install --upgrade matplotlib\n",
        "#!pip install --upgrade pandas\n",
        "#!pip install --upgrade scikit-learn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "#!pip install tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooFUyCHmFS6x",
        "outputId": "4c05f97f-3e1e-4200-d713-6aee57d9b08a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.18.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
            "Required-by: dopamine_rl, tf_keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load Dataset"
      ],
      "metadata": {
        "id": "hOq1v_GgGFgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/Twitter_Data.csv\")"
      ],
      "metadata": {
        "id": "lWUkDuIzGF8_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values and drop them\n",
        "df.dropna(subset=['clean_text', 'category'], inplace=True)"
      ],
      "metadata": {
        "id": "X3r074UjGWVF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. Text Preprocessing"
      ],
      "metadata": {
        "id": "ZHYpgjPqGzdn"
      }
    },
    {
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data packages\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the missing punkt_tab data package\n",
        "\n",
        "# Pre-compile regular expression\n",
        "pattern = re.compile(r'[^a-zA-Z\\s]')\n",
        "\n",
        "# Create a set of stopwords for faster lookup\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    text = pattern.sub('', text)  # Use pre-compiled pattern\n",
        "\n",
        "    # Tokenize the text and remove stopwords in one step\n",
        "    tokens = [word for word in word_tokenize(text) if word not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['cleaned_text'] = df['clean_text'].apply(preprocess_text)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVBYFRt6Azs7",
        "outputId": "5770550c-93d7-481b-9b5d-0bdb310f6d09"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Feature Engineering"
      ],
      "metadata": {
        "id": "Ih3IfuhiPJfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_tfidf = vectorizer.fit_transform(df['cleaned_text']).toarray()\n",
        "y = df['category']"
      ],
      "metadata": {
        "id": "fGyC36XQPKey"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Split Dataset"
      ],
      "metadata": {
        "id": "ONjQJzM0PR0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "lnbr6t2lPSdU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Incremental PCA for dimensionality reduction\n",
        "n_batches = 10  # Adjust based on your data size and memory capacity\n",
        "# Set n_components to an integer or None\n",
        "inc_pca = IncrementalPCA(n_components=None, # or a specific number of components, e.g., 100\n",
        "                         batch_size=X_train.shape[0] // n_batches)\n",
        "\n",
        "X_train_pca = inc_pca.fit_transform(X_train)\n",
        "X_test_pca = inc_pca.transform(X_test)\n"
      ],
      "metadata": {
        "id": "bcqzYaDJm0R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Machine Learning Models"
      ],
      "metadata": {
        "id": "Ji-1D73WPtIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes"
      ],
      "metadata": {
        "id": "wAORFMeTPwc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "y_pred_nb = nb_model.predict(X_test)"
      ],
      "metadata": {
        "id": "PeQvkDbePw62"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machine"
      ],
      "metadata": {
        "id": "YzlBTIYrP1S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model = SVC(kernel='linear', probability=True)\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred_svm = svm_model.predict(X_test)"
      ],
      "metadata": {
        "id": "ZRgHXbo-P5h6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}